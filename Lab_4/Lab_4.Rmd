---
title: "Lab 4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style type='text/css'>
  body{
  font-size: 14pt;
  }
  pre {
  font-size: 12pt;
  }
</style>

```{r library, message=F}
library(tidyverse)
library(car)
```

In this lab we will be covering the **General Linear Model** (GLM) and its relationship to ANOVAs. We'll cover implementing the GLM and an ANOVA by hand, and compare their outputs to functions built in R. For modeling, we'll work with a data set that contains information on 76 people who undertook one of three diets (*A*, *B*, *C*). The data set also includes background information such as age, gender, and height. Lets jump into it!

# Load Data

The dataset includes comlumns for:

+ `Person` ID
+ `gender`
+ `Age`
+ `Height`
+ `pre.weight` - weight at the start of the weight loss program
+ `Diet` - type of diet subject was put on
+ `weight6weeks` - weight at the end of the 6 week program


We'll just speed through the data preprocessing stage, but if you want a more extensive break down of data wrangling and exploratory data analysis refer to Lab 3. Here, we are using `tidyverse` to quickly 
create a new column for weight loss, convert categorical variable to factors, and remove rows with missing values.

```{r load & process data}
diet.df <- read.csv('diet.csv')

diet.df <- diet.df %>% 
  rename(final.weight = weight6weeks) %>% # rename weight6weeks column to final.weight
  mutate(weight.loss = final.weight - pre.weight,
         Diet = factor(Diet, labels=c('A','B','C')),
         gender = factor(gender, labels=c('F','M')),
         Person = factor(Person)) %>% 
  drop_na() # drop missing data

head(diet.df)
```

# General Linear Model

As presented in lecture, the GLM is realized in the following equation:

$$
\begin{aligned}
\underline{y} = X\underline{\beta} + \underline{\epsilon}
\end{aligned}
$$
where $\underline{y}$ is an $n \times 1$ vector of dependent variables, $X$ is the $n \times p$ design matrix, $\underline{\beta}$ is a $p \times 1$ vector of unknown parameters to be estimated, and $\underline{\epsilon}$ is an $n \times 1$ vector.   

This model is the work horse of statistics, and the basis of some machine/deep learning algorithms. Essentially, it states that the dependent variable **y** has a linear relationship with with some predictors that are represented by the design matrix. Using the matrix algebra that was stressed so heavily these past three weeks, parameters in the $\beta$ vector can be estimated, thereby producing a quantitative description of the linear relationship.   

I won't take you through the whole derivation of how we can estimate these parameters, but just believe that the best estimator of $\beta$ is:

$$
\begin{aligned}
\underline{\hat{\beta}} = (X'X)^{-1}X'\underline{y}
\end{aligned}
$$
Note, the carrot on top of $\hat{\beta}$ indicates that this is an *estimation* for the true value of $\beta$. This method is known as **Ordinary Least Squares** (OLS) and in the best case scenario (i.e. all model assumptions are met), OLS estimates in $\hat{\beta}$ are the Best Linear Unbiased Estimators (BLUE) of parameters in $\beta$. In plain English, the estimated values have the minimum amount of variance compared to all other linear and unbiased estimators.   

With this equation, we can proceed to use the GLM by hand to derive estimates for the parameters in $\beta$.

## GLM By Hand

First we'll define the relationship we are interested in testing. In this lab, we want to see if there are any differences in weight loss across the different diets. Setting up this analysis with the GLM, we have the following equation:

$$
\begin{aligned}
\underline{y} = \begin{bmatrix}
y_{11} \\
y_{12} \\
y_{21} \\
y_{22} \\
y_{31} \\
y_{32}
\end{bmatrix}
 
X = \begin{bmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1 \\
1 & 0 & 1  
\end{bmatrix}

\underline{\beta} = \begin{bmatrix}
\mu \\
\tau_{1} \\
\tau_{2} 
\end{bmatrix}

\underline{\epsilon} = \begin{bmatrix}
\epsilon_{11} \\
\epsilon_{12} \\
\vdots \\
\epsilon_{32}
\end{bmatrix}
\end{aligned}
$$

At the single subject level, this equation is:

$$
\begin{align*}
y_{11} = \mu + \epsilon_{11} \\
y_{21} = \mu + \tau_{1} + \epsilon_{21} \\
y_{31} = \mu + \tau_{2} + \epsilon_{31} \\
\end{align*}
$$
We can create the design matrix in R using the function `model.matrix`. Passed into the input is the model formula `weight.loss ~ Diet`, which states that we want to look at the linear relationship between weight loss and the different types of diets.  

Note, on the left side of the `~` is your response variable (i.e., *y*), and the right side is your input variables.
```{r Design matrix and outcome}
X <- model.matrix(weight.loss ~ Diet, data=diet.df)
y <- diet.df$weight.loss

head(X)

tail(X)
```
Notice that the first column of the design matrix is `(Intercept)` rather than `DietA`. By default, R sets the first level of a categorical variable as the intercept in the model design matrix. This can be overridden by removing the intercept term (e.g., `weight.loss ~ -1 + Diet`). Its important to know how R codes your categorical variables in the model, because this will influence their interpretation.   

Now, using the OLS equation for $\beta$ above, we can compute its estimates. Here, we use the function `crossprod` to compute the cross product between two matrices. If a single matrix is passed to the function, then its the cross product between the matrix and its transpose. The `solve` function then solves a system of equations.

```{r Outcome variable}
beta <- solve(crossprod(X), crossprod(X,y))

print(beta)
```
Looking at the estimated coefficients, the `(Intercept)` term represents the **mean value for the first group** (Diet A). The coefficients for Diet B and C represent the deviation of these groups from the mean of Diet A. So, the mean of Diet B = `beta[1] + beta[2]` = `r beta[1]+beta[2]`. 

The $\epsilon$ term in the GLM represents the amount of error between model predictions (typically denoted as $\hat{y}$) and the actual data. You can think of this as how well the estimated parameters of $\beta$ fit the data, how much <u>residual</u> heterogeneity was left unexplained. Model predictions can be computed as:

$$
\begin{aligned}
\hat{y} = X\hat{\beta}
\end{aligned}
$$
Doing this in R:

```{r residuals}
error <- y - X%*%beta

head(error)
```
Great! Now that we've done it by hand, lets use R packages that take care of this and more.

## Working with `lm()`

The `lm()` function is typically used for linear regression. You'll come to see that an ANOVA is just a special case of regression with categorical variables, but we can use this function to estimate the beta coefficients. When using the function, we'll use the same formula as before:

```{r lm}
model.1 <- lm(weight.loss ~ Diet, data=diet.df)

summary(model.1)
```
The model summary shows that the estimated coefficients are the same as what we computed by hand before! Now, we have the added bonus of seeing the amount of error in those estimates, if they were significantly different from the null value of zero, and if the overall model fit was significant.   

We can even compare the residuals we computed by hand to the model residuals.
```{r}
error.df <- cbind(error,model.1$residuals)
head(error.df)
```
And they are exactly the same.